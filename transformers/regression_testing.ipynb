{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationArgs, ClassificationModel\n",
    "from sklearn.metrics import root_mean_squared_error, cohen_kappa_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# general hyperparameters for the execution\n",
    "hyperparams = {\n",
    "    \"TARGET\": 5, # Competences 1-5; (Global Score 6 - not used)\n",
    "    \n",
    "    \"MANUAL_SEED\": 42,\n",
    "    \"NUM_EPOCHS\": 6,\n",
    "    \"LEARNING_RATE\": 2e-5,\n",
    "    \"WEIGHT_DECAY\": 0.01,\n",
    "    \"TRAIN_BATCH_SIZE\": 8,\n",
    "    \"EVAL_BATCH_SIZE\": 8,\n",
    "    \"GRADIANT_ACCUMULATION_STEPS\": 1,\n",
    "    \"EVAL_DURING_TRAIN_STEPS\": 30,\n",
    "    \"EARLY_STOP_MET\": \"eval_loss\",\n",
    "    \"EARLY_STOP_PAT\": 2,\n",
    "\n",
    "    \"REGRESSION\": True,\n",
    "    # \"MAX_SEQ_LENGTH\": 256,\n",
    "    \"NO_CACHE\": False,\n",
    "    \"LOGGING_STEPS\": 20,\n",
    "}\n",
    "\n",
    "# output and model paths\n",
    "OUTPUT_PATH = f\"../outputs/transformers/c{hyperparams['TARGET']}\"\n",
    "BEST_MODEL_PATH = f\"{OUTPUT_PATH}/best-model\"\n",
    "LOG_PATH = f\"../resultados/c{hyperparams['TARGET']}\"\n",
    "\n",
    "# model arguments\n",
    "MODEL_TYPE = \"bert\"\n",
    "MODEL_VARIATION = \"base\" # base/large\n",
    "MODEL_NAME = f\"neuralmind/{MODEL_TYPE}-{MODEL_VARIATION}-portuguese-cased\"\n",
    "MODEL_ARGS = ClassificationArgs(\n",
    "    manual_seed=hyperparams[\"MANUAL_SEED\"],\n",
    "    output_dir=OUTPUT_PATH,\n",
    "    best_model_dir=BEST_MODEL_PATH,\n",
    "    overwrite_output_dir=True,\n",
    "    reprocess_input_data=True,\n",
    "    use_multiprocessing=False,\n",
    "    evaluate_during_training=True,\n",
    "    evaluate_each_epoch=True,\n",
    "    save_best_model=True,\n",
    "    use_early_stopping=True,\n",
    "    save_eval_checkpoints=False,\n",
    "    save_model_every_epoch=False,\n",
    "    regression=hyperparams[\"REGRESSION\"],\n",
    "    # max_seq_length=hyperparams[\"MAX_SEQ_LENGTH\"],\n",
    "    no_cache=hyperparams[\"NO_CACHE\"],\n",
    "    logging_steps=hyperparams[\"LOGGING_STEPS\"],\n",
    "    num_train_epochs=hyperparams[\"NUM_EPOCHS\"],\n",
    "    learning_rate=hyperparams[\"LEARNING_RATE\"],\n",
    "    weight_decay=hyperparams[\"WEIGHT_DECAY\"],\n",
    "    train_batch_size=hyperparams[\"TRAIN_BATCH_SIZE\"],\n",
    "    eval_batch_size=hyperparams[\"EVAL_BATCH_SIZE\"],\n",
    "    gradient_accumulation_steps=hyperparams[\"GRADIANT_ACCUMULATION_STEPS\"],\n",
    "    evaluate_during_training_steps=hyperparams[\"EVAL_DURING_TRAIN_STEPS\"],\n",
    "    early_stopping_metric=hyperparams[\"EARLY_STOP_MET\"],\n",
    "    early_stopping_patience=hyperparams[\"EARLY_STOP_PAT\"],\n",
    ")\n",
    "\n",
    "# dataset path and definition\n",
    "DATASET_PATH = \"../corpus/\"\n",
    "DATASET_NAME = \"-00000-of-00001.parquet\"\n",
    "DIVISIONS = (\"train\", \"validation\", \"test\")\n",
    "def target_dataset_path(target: str):\n",
    "    if target in DIVISIONS:\n",
    "        return DATASET_PATH + target + DATASET_NAME\n",
    "    else:\n",
    "        raise ValueError(\"ERROR: Invalid target for dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5af975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map grades\n",
    "def normalize_score(row, target):\n",
    "    return row['grades'][target] // 40\n",
    "\n",
    "essays_set = {}\n",
    "for division in DIVISIONS:\n",
    "    essays = pd.read_parquet(target_dataset_path(division), engine='pyarrow')[['essay_text', 'grades']]\n",
    "    essays.rename(columns={'essay_text': 'text'}, inplace=True)\n",
    "    essays['label'] = essays.apply(lambda row: normalize_score(row, hyperparams[\"TARGET\"] - 1), axis=1) # Normalizing target score\n",
    "    essays_set[division] = essays[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_name=MODEL_NAME,\n",
    "    num_labels=1, # regression\n",
    "    args=MODEL_ARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step, training_details = model.train_model(essays_set['train'], eval_df=essays_set['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train loss\n",
    "train_loss_df = pd.DataFrame({\n",
    "    'step': training_details['global_step'],\n",
    "    'running_loss': training_details['train_loss']\n",
    "})\n",
    "\n",
    "print('Training Loss History:')\n",
    "print(train_loss_df)\n",
    "\n",
    "# get evaluation loss\n",
    "eval_steps = []\n",
    "if 'eval_loss' in training_details and training_details['eval_loss']:\n",
    "    initial_step = model.args.evaluate_during_training_steps\n",
    "    num_evals = len(training_details['eval_loss'])\n",
    "    eval_steps = [initial_step * (i + 1) for i in range(num_evals)]\n",
    "\n",
    "eval_loss_df = pd.DataFrame({\n",
    "    'step': eval_steps,\n",
    "    'eval_loss': training_details.get('eval_loss', []) # .get() para evitar erro se a lista estiver vazia\n",
    "})\n",
    "\n",
    "print('Evaluation Loss History:')\n",
    "print(eval_loss_df)\n",
    "\n",
    "# make graph to analyze\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(train_loss_df['step'], train_loss_df['running_loss'], label='Train Loss (Running Loss)')\n",
    "if not eval_loss_df.empty:\n",
    "    plt.plot(eval_loss_df['step'], eval_loss_df['eval_loss'], label='Validation Loss (Eval Loss)', marker='o', linestyle='--')\n",
    "\n",
    "plt.title('Learning Curve: Train Loss vs. Validation Loss', fontsize=16)\n",
    "plt.xlabel('Training Steps', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "best_model = ClassificationModel(\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_name=BEST_MODEL_PATH,\n",
    "    num_labels=1, # regression\n",
    "    args=MODEL_ARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5247726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently using best_model.predict()\n",
    "# result, model_outputs, wrong_predictions = best_model.eval_model(essays_set[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enem_accuracy_score(true_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Calcula a proporção de predições que estão dentro do limite de 80 pontos de divergência em relação às notas verdadeiras, conforme os critérios de avaliação do ENEM.\n",
    "\n",
    "    true_values: Lista de notas verdadeiras (valores reais).\n",
    "    predicted_values: Lista de notas preditas pelo modelo.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(true_values) == len(predicted_values), \"Mismatched length between true and predicted values.\"\n",
    "\n",
    "    # non_divergent_count = sum([1 for t, p in zip(true_values, predicted_values) if abs(t - p) <= 80])\n",
    "    non_divergent_count = sum([1 for t, p in zip(true_values, predicted_values) if abs(t - p) <= 2]) # 80 -> 2\n",
    "\n",
    "    return non_divergent_count / len(true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = essays_set['test']['text'].tolist()\n",
    "y_true = essays_set['test']['label'].tolist()\n",
    "\n",
    "y_pred, raw_outputs = best_model.predict(test_text_list)\n",
    "y_pred_round = np.clip(np.round(y_pred), 0, 5).astype(int)\n",
    "\n",
    "# get metrics\n",
    "qwk = cohen_kappa_score(y_true, y_pred_round, weights='quadratic')\n",
    "enem_acc = enem_accuracy_score(y_true, y_pred_round)\n",
    "class_rep_mtx = classification_report(y_true, y_pred_round, target_names=['Nota 0', 'Nota 40', 'Nota 80', 'Nota 120', 'Nota 160', 'Nota 200'], zero_division=0.0)\n",
    "conf_mtx = confusion_matrix(y_true, y_pred_round)\n",
    "\n",
    "# scale for RMSE metric\n",
    "scaled_predictions = y_pred * 40\n",
    "scaled_original = np.array(y_true) * 40\n",
    "rmse = root_mean_squared_error(scaled_original, scaled_predictions)\n",
    "\n",
    "print(f'Quadratic Weighted Kappa: {qwk:.4f}\\n')\n",
    "print(f'ENEM Accuracy: {enem_acc:.4f}\\n')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}\\n')\n",
    "print('Classification Report Matrix:')\n",
    "print(class_rep_mtx)\n",
    "print('\\nConfusion Matrix:')\n",
    "print(conf_mtx)\n",
    "\n",
    "# precision (for each class): % of predictions for that were actually correct\n",
    "# recall (for each class): % of all actual instances that the model correctly identified\n",
    "# f1-score (for each class): harmonic mean of precision and recall - performance\n",
    "# support (for each class): total number of actual occurrences in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166307ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up path and file for report\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_filename = f\"transformer_regressao_c{hyperparams['TARGET']}_{timestamp}.txt\"\n",
    "full_log_path = os.path.join(LOG_PATH, log_filename)\n",
    "os.makedirs(LOG_PATH, exist_ok=True)\n",
    "\n",
    "# report informations\n",
    "report_content = f\"\"\"\n",
    "======================================================================\n",
    "           RELATÓRIO DE EXPERIMENTO DE MODELO\n",
    "======================================================================\n",
    "\n",
    "Data e Hora: {timestamp}\n",
    "Arquivo de Log: {log_filename}\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "                   INFORMAÇÕES DO MODELO\n",
    "----------------------------------------------------------------------\n",
    "\"\"\"\n",
    "report_content += f\"{'MODEL_TYPE':<28}: {MODEL_TYPE}\\n\"\n",
    "report_content += f\"{'MODEL_VARIATION':<28}: {MODEL_VARIATION}\\n\"\n",
    "report_content += f\"{'MODEL_NAME':<28}: {MODEL_NAME}\\n\"\n",
    "report_content += \"\"\"\n",
    "----------------------------------------------------------------------\n",
    "               HIPERPARÂMETROS DE TREINAMENTO\n",
    "----------------------------------------------------------------------\n",
    "\"\"\"\n",
    "for key, value in hyperparams.items():\n",
    "    report_content += f\"{key:<28}: {value}\\n\"\n",
    "report_content += \"\"\"\n",
    "----------------------------------------------------------------------\n",
    "                      HISTÓRICO DE LOSS\n",
    "----------------------------------------------------------------------\n",
    "\"\"\"\n",
    "report_content += \"\\nTraining Loss History:\\n\"\n",
    "report_content += train_loss_df.to_string()\n",
    "report_content += \"\\n\\nEvaluation Loss History:\\n\"\n",
    "report_content += eval_loss_df.to_string()\n",
    "report_content += \"\"\"\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "             MÉTRICAS DE AVALIAÇÃO (CONJUNTO DE TESTE)\n",
    "----------------------------------------------------------------------\n",
    "\"\"\"\n",
    "report_content += f\"\\nQuadratic Weighted Kappa: {qwk:.4f}\\n\"\n",
    "report_content += f\"ENEM Accuracy: {enem_acc:.4f}\\n\"\n",
    "report_content += f\"Root Mean Squared Error: {rmse:.4f}\\n\"\n",
    "report_content += \"\\nClassification Report:\\n\"\n",
    "report_content += class_rep_mtx  # already string\n",
    "report_content += \"\\n\\nConfusion Matrix:\\n(Linhas = Real, Colunas = Previsto)\\n\"\n",
    "report_content += str(conf_mtx) # numpy to string\n",
    "report_content += \"\"\"\n",
    "\n",
    "======================================================================\n",
    "                       FIM DO RELATÓRIO\n",
    "======================================================================\n",
    "\"\"\"\n",
    "\n",
    "with open(full_log_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"\\nRelatório do experimento salvo com sucesso em: '{full_log_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
